{
  "prune_step": {
    "title": "Pruning the SNPs",
    "description": "",
    "error": ""
  },
  "reduce_prune": {
    "title": "Pairwise SNP pruning",
    "description": "Pruning SNPs to a minimal set by removing correlated SNPs within a sliding window. This step speeds up the ML model training and reduces possible bias due to overfitting of the subsequent models.",
    "error": "Failure at this step usually is due to too few SNPs and/or everything being closely correlated, which is a pretty weird scenario in general and really shouldn’t happen."
  },
  "reduce_prsice": {
    "title": "SNP pruning using PRSiceV2 workflow",
    "description": "Using PRSiceV2 workflow, SNPs are selected here as if a polygenic risk score was being constructed utilizing external GWAS summary statistics to guide SNP selection.",
    "error": "The most common failure at this step is formatting problems with the external GWAS summary statistics. The biggest problem people often have here is not using matching SNP names … are your SNPs using RS IDs and the GWAS summary stats using CHR:BP IDs?"
  },
  "reduce_sblup": {
    "title": "SNP pruning using SBLUP workflow",
    "description": "Using SBLUP workflow, SNPs are selected utilizing external GWAS summary statistics as well as a heritability estimate to guide SNP selection.",
    "error": "The most common failure at this step is formatting problems with the external GWAS summary statistics. The biggest problem people often have here is not using matching SNP names … are your SNPs using RS IDs and the GWAS summary stats using CHR:BP IDs?"
  },
  "merge_reduced": {
    "title": "Merging input dataset for model training",
    "description": "Merging all files containing participant level data. Only samples in all files specified will be retained here. Genotypes have been reduced in the previous pruning step.",
    "error": "The likely points of failure here are ignoring the file formatting conventions and having sample IDs that do not overlap across files."
  },
  "train_step": {
    "title": "Training the ML model",
    "description": "Evaluating multiple machine learning methods. The method with the best mean cross-validation performance will be selected.",
    "error": "The main failure points are simply hardware related at this phase of work. Is your data way too big for your computer? Also, the code implemented here occasionally bugs if you have too many zero variance predictors in the dataset, but you probably already removed those before starting your analyses, right?"
  },
  "tune_step": {
    "title": "Tuning the ML model",
    "description": "You have successfully selected an algorithm and built a model in the previous phase, now it is time to tune it and gain a little performance.",
    "error": "At testing, the only failures we saw at this phase were hardware related. Once again, how big is your computer compared to your data?"
  },
  "validation_step": {
    "title": "Validation of ML Model",
    "description": "Validation is the real indicator of how well you can generalize the model you built in the previous phases of analysis.",
    "error": "There are a few sources of failure that are pretty common here. Generally check column names for all the predictors to make sure they mirror those in the input files for the training data. Make sure that all of the data points input into the training data as possible predictors are also in the validation data. Finally, make sure everything is on the same scale i.e. don’t use a phenotype of cells per liter in training and then cells per deciliter in the validation, you won’t be happy with the results."
  },
  "check_dependencies": {
    "title": "Dependency Check",
    "description": "",
    "error": ""
  }
}