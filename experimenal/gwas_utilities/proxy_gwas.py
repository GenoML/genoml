# -*- coding: utf-8 -*-
"""proxy_GWAS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x-rk7tQvgMzRnX1Rl7WmRRSPtqaTJTUU

# Intro
 - **Project:** proxy_GWAS
 - **Draft author(s):** Mike Nalls and Mary Makarious
 - **Tester(s):** Hampton Leonard and Hirotaka Iwaki
 - **Date Notebook Started:** 16.10.2019
    - **Quick Description:** Notebook to convert proxy GWAS results to GWAS meta-analysis compatible results.

---
### Quick Description: 
**Problem:** Biobanks are a major part of our future work. Although summary stats from case proxies such as 1st degree family history reported individuals are not directly comparable to traditional case-control designs.

**Solution:** Lets adjust these summary stats to be scaled for GWAS meta-analyses. 

### Motivation/Background:
Necessary for GWAS meta-analyses.
See these papers:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5959890/
Our method mirrors Llyod Jones which was used in the IPDGC 2019 meta-gwas and can be found here https://www.ncbi.nlm.nih.gov/pubmed/29429966/

### Concerns/Impact on Related Code: 
- Input summary statistics must be correctly formatted and from a proxy-case analysis.

### Thoughts for Future Development of Code in this Notebook: 
- Would be great to do this on the fly using HAIL [https://hail.is/].

### Testing notes: 
- Tested against http://cnsgenomics.com/shiny/LMOR/.

# Imports
"""

import argparse
import sys
import xgboost
import sklearn
import h5py
import pandas as pd
import numpy as np
import time
import math
import scipy

"""# Insert options for testing in notebook"""

# run_prefix = "test_data.csv"
# beta_name = 'b'
# se_name = 'se'
# p_name = 'p'
# freq_name = 'Freq'
# k_prevalence = 0.2
# scalar = 2
# outfile = "test_data_outfile.csv"

# import os
# from google.colab import drive
# drive.mount('/content/drive/')
# os.chdir("/content/drive/Shared drives/LNG/projects/proxy_GWAS/")
# ! pwd

"""# Command args"""

parser = argparse.ArgumentParser(description='Arguments for training a discrete model')    
parser.add_argument('--input-file', type=str, default='GenoML_data', help='Path to your GWAS summary stats data for conversion in comma separated format, should be from logistic regression on family history versus no family history or similar.')
parser.add_argument('--beta-proxy', type=str, default='b', help='Name of beta column.')
parser.add_argument('--se-proxy', type=str, default='se', help='Name of stadnard error column.')
parser.add_argument('--p-proxy', type=str, default='p', help='Name of p vlaue column.')
parser.add_argument('--pop-freq', type=str, default='freq', help='Name of the effect allele frequency in your population.')
parser.add_argument('--prevalence', type=float, default=0.5, help='Prevalence of the disease, value between 0 and 1.')
parser.add_argument('--scalar', type=int, default=1, help='Beta coefficient scalar, 1 generally for mixed models, 2 for proxy case logistic regression.')
parser.add_argument('--outfile', type=str, default=1, help='Output CSV path and intended name.')

args = parser.parse_args()

print("")
print("Here is some basic info on the command you are about to run.")
print("Python version info...")
print(sys.version)
print("CLI argument info...")
print("Working with dataset", args.input_file, " ... these are youyr GWAS summary stats from the proxy case comparison.")
print("What's your beta coefficient column named?", args.beta_proxy)
print("What's your standard error column named?", args.se_proxy)
print("What's your p value column named?", args.p_proxy)
print("What's your effect allele frequency column named?", args.pop_freq)
print("What's the prevalence of your disease in this population?", args.prevalence)
print("What's the scalar you are using?", args.scalar)
print("Give credit where credit is due, this work is based on these papers https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5959890/, https://www.ncbi.nlm.nih.gov/pubmed/29429966/ and  .")
print("")

run_prefix = args.input_file
beta_name = args.beta_proxy
se_name = args.se_proxy
p_name = args.p_proxy
freq_name = args.pop_freq
k_prevalence = args.prevalence
scalar = args.scalar
outfile = args.outfile

"""# Read in your data, generally a csv file and summarize"""

df = pd.read_csv(run_prefix, engine = "c")

print("Top few lines fo your file...")
print(df.head())

print("Quick summary of your input data...")
print(df.describe())

print("Quick summary of your data types...")
print(df.dtypes)

"""# Rename columns to b, se, p and freq"""

df.rename(columns={beta_name:'b', se_name:'se', p_name:'p', freq_name:'freq'}, inplace=True)

#df.head(3)

"""# Generate adjusted odds ratio and return adjusted beta"""

print("Rescaling if specified, generally 1 for mixed model and 2 for proxy GWAS.")

def fx_b_scaling(x):
  return x*scalar

df['b_scaled'] = np.vectorize(fx_b_scaling)(df['b'])

print("Calculating raw (proxy) odds ratio.")

def fx_proxy_OR(x):
  return math.exp(x)

df['OR_raw'] = np.vectorize(fx_proxy_OR)(df['b_scaled'])

print("Calculate adjusted odds ratios using frequency and prealence estimates. Note, the beta used in this calc is multiplied by two to rescale for classic case-control study compatability.")

def fx_scale_OR(b, p, k):
  upper = (k+(b*(1-p)))*(1-k+(b*p))
  lower = (k-(b*p))*(1-k-(b*(1-p)))
  return upper/lower

df['OR_adjusted'] = np.vectorize(fx_scale_OR)(df['b_scaled'], df['freq'], k_prevalence)

print("Now calculating the adjusted beta derived from the adjusted odds ratio.")

def fx_log_OR(o):
  b_adjusted = np.log(o)
  return b_adjusted

df['b_adjusted'] = np.vectorize(fx_log_OR)(df['OR_adjusted'])

df.iloc[0:21, 1:13]

"""# Generate approximated standard error from adjusted beta and P"""

# Fix floating point errors from small P values in GWAS

print("Now deriving the adjusted SE from the adjusted beta.")

def fx_beta_p_to_SE(b, p):
  if (p < 1E-15):
    p = 1E-15
    z_score = scipy.stats.norm.ppf(1 - (p/2))
  if (p >= 1E-15):
    z_score = scipy.stats.norm.ppf(1 - (p/2))
  se_adj = abs(b/z_score)
  return se_adj

df['se_adjusted'] = np.vectorize(fx_beta_p_to_SE)(df['b_adjusted'], df['p'])

print("Quick sanity check for P derived from adjusted stats. NOTE: \"we keep it real\" regarding floats, so we are capping estimates at 15 decimal places, so there may be some conservative estiamtes but after p < 1E-15 does that really matter?")

def fx_test_P(b, se):
  z_derived = b/se
  p_derived = scipy.special.ndtr(-1 * abs(z_derived)) * 2
  return p_derived

df['p_derived'] = np.vectorize(fx_test_P)(df['b_adjusted'], df['se_adjusted'])

print("For meta-analysis in METAL and similar, please use *_adjusted columns.")

# df.iloc[0:25, 1:18]

"""# Export same dataset with adjusted beta, SE and OR"""

print("Top few lines of your output file...")
print(df.head())

print("Quick summary of your out data...")
print(df.describe())

print("Quick summary of your output data types...")
print(df.dtypes)

df.to_csv(outfile, index=False)
